# DisTrO

This is the repository for DisTrO (Distributed Training Over-The-Internet), a family of low latency distributed optimizers that reduce inter-GPU communication requirements by three to four orders of magnitude.

- [x] Aug. 26th, 2024: DisTrO [(Preliminary Report)](https://github.com/NousResearch/DisTrO/raw/main/A_Preliminary_Report_on_DisTrO.pdf)
- [x] Dec. 2nd, 2024: DeMo: Decoupled Momentum Optimization, both [Paper](https://arxiv.org/abs/2411.19870) and [Reference Implementation](https://github.com/bloc97/DeMo)
- [x] Dec. 2nd, 2024: [Nous trains a 15b model using DisTrO](https://distro.nousresearch.com/)
- [ ] In The Near Future: ðŸ‘€

[Join us on Discord](https://discord.com/invite/jqVphNsB4H) if you're interested in helping research and build the future of distributed training.
