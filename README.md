# DisTrO
This is the repository for DisTrO, a family of architecture and network-agnostic distributed optimizers that reduce inter-GPU communication requirements by four to five orders of magnitude without relying on amortized analysis, enabling low-latency training of large neural networks on slow internet bandwidths with heterogeneous networking hardware.

- [x] Aug. 26th, 2024: [Preliminary Report](./A_Preliminary_Report_on_DisTrO.pdf)
- [ ] Coming Soon: Paper and Code
- [ ] In The Near Future: ðŸ‘€

[Join us on Discord](https://discord.com/invite/jqVphNsB4H) if you're interested in helping research and build the future of distributed training.
